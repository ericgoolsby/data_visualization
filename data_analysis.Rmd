---
title: "Example statistical tests"
author: "Eric Goolsby"
date: "11/18/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Common statistical test are linear models

If you want to learn a bit more about linear model-based tests (t-test, ANOVA, ANCOVA, chi-squared test, non-parameteric tests), check out [Common statistical tests are linear models: a work through](https://steverxd.github.io/Stat_tests/linearmodel.html). It provides a nice summary of what these tests do, describes how they are just special cases of linear models, and includes example R code.

## [Data Analysis and Statistics with R](https://dzchilds.github.io/stats-for-bio)

Another great resource with worked examples is [Data Analysis and Statistics with R](https://dzchilds.github.io/stats-for-bio) (by Dylan Z. Childs, Bethan J. Hindle and Philip H. Warren). You can load any of the referenced datasets by first creating the `get_data` function:
```{r}
get_data <- function(filename) read.csv(paste("https://raw.githubusercontent.com/dzchilds/stats-for-bio/master/data_csv/",filename,sep=""))
```

Then you can simply run something like this: `data_anme <- get_data("filename.csv")`. For example, the t-test example refers to a data frame named `morph_data`, loaded from `MORPH_DATA.CSV`:
```{r}
morph_data <- get_data("MORPH_DATA.CSV")
head(morph_data)
```

- Is my continuous variable different from zero? [one-sample t-test](https://dzchilds.github.io/stats-for-bio/one-sample-t-tests.html)
- Are my two continuous variables statistically different from one another? [two-sample t-test](https://dzchilds.github.io/stats-for-bio/two-sample-t-test.html)
- Are two continuous variables correlated?
    - [correlation test](https://dzchilds.github.io/stats-for-bio/a-correlation-test.html)
    - [relationships and regression](https://dzchilds.github.io/stats-for-bio/relationships-and-regression.html)
    - [simple regression](https://dzchilds.github.io/stats-for-bio/simple-regression-in-r.html)
- I have continuous variables from multiple groups (i.e., like t-test but with more than two groups). Are the groups statistically different?
    - [one-way ANOVA introduction](https://dzchilds.github.io/stats-for-bio/introduction-to-one-way-anova.html)
    - [one-way ANOVA](https://dzchilds.github.io/stats-for-bio/one-way-anova-in-r.html)
- t-test, but with paired variables: [paired-sample t-test](https://dzchilds.github.io/stats-for-bio/paired-sample-t-test.html)
- I need to perform an ANOVA, but my experimental design included blocks. [ANOVA for randomied block designs](https://dzchilds.github.io/stats-for-bio/anova-for-randomised-block-designs.html)
- I want to test for an interaction effect between two variables.
    - [intro to two-way ANOVA](https://dzchilds.github.io/stats-for-bio/two-way-anova-intro.html)
    - [two-way ANOVA](https://dzchilds.github.io/stats-for-bio/two-way-anova-in-r.html)
- ANOVA, but controlling for a nuisance variable or a covariate.
     - [intro to ANCOVA](https://dzchilds.github.io/stats-for-bio/introduction-to-ancova.html)
     - [two-way ANCOVA](https://dzchilds.github.io/stats-for-bio/two-way-ancova-in-r.html)
- Model assumptions and diagnostics
    - [Assumptions and diagnostics](https://dzchilds.github.io/stats-for-bio/assumptions-diagnostics.html)
    - [Regression diagnostics](https://dzchilds.github.io/stats-for-bio/regression-diagnostics.html)
    - [Data transformations](https://dzchilds.github.io/stats-for-bio/data-transformations.html)
    - [Multiple comparison tests](https://dzchilds.github.io/stats-for-bio/multiple-comparison-tests.html)
- Counts, frequenccies, and non-parametric data
    - [Frequencies](https://dzchilds.github.io/stats-for-bio/working-with-frequencies.html)
    - [Goodness of fit tests](https://dzchilds.github.io/stats-for-bio/goodness-of-fit-tests.html)
    - [Contingency tables](https://dzchilds.github.io/stats-for-bio/contingency-tables.html)
    - [Non-parametric tests](https://dzchilds.github.io/stats-for-bio/non-parametric-tests.html)

## Exploratory relationships

We're going to use the `GGally` package to visualize pairwise relationships, and the `easystats` packages to make data analysis easier.

```{r}
#install.packages("GGally")
library(GGally)

#install.packages("easystats", repos = "https://easystats.r-universe.dev")
library(easystats)
```

### Exploring correlations between multiple continuous variables

Let's summarize correlations in the iris dataset, without accounting for species:

```{r}
iris_cor <- correlation(iris)

# view the results
iris_cor

## or in matrix format
summary(iris_cor)
```

Now let's visualize the correlations using a few different (redundant) approaches:

```{r}
ggpairs(iris,columns = 1:4)
```

```{r}
iris_cor %>% summary(redundant = TRUE) %>% plot()
```

In base R, we can accomplish something similar. Optionally, first define a helper function called `panel.cor`:

```{r}
panel.cor <- function(x, y, digits = 2, prefix = "", size_by_cor = FALSE, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    
    text(0.5, 0.5, txt,cex = if(size_by_cor) 0.8/strwidth(txt) else FALSE)#, cex = cex.cor * r)
}
```

Then make the pairwise plot with or without the helper function:

```{r}
# without the helper function panel.cor
pairs(iris[,1:4])

# with the helper function panel.cor
pairs(iris[,1:4],upper.panel = panel.cor)
```

### Exploring correlations between multiple continuous variables with groups (e.g. Species)

```{r}
ggpairs(iris, columns = 1:4, aes(color = Species, alpha = 0.5),
        lower = list(continuous = "smooth"))
```

We can't plot by groups using this approach. Still, it's useful:
```{r}
iris_cor_grouped <- iris %>% 
  group_by(Species) %>%
  correlation()

iris_cor_grouped

summary(iris_cor_grouped)
```


**Similar, but in base R:**

```{r}
pairs(iris[,1:4], col = hcl.colors(length(unique(iris$Species)), "Temps")[iris$Species],pch=19,upper.panel = panel.cor)
```


### Exploring multiple continuous and discrete variables

```{r}
ggpairs(iris, columns = 1:5, aes(color = Species, alpha = 0.5),
        lower = list(continuous = "smooth"))
```

## Linear regression


### Palmer penguins dataset

```{r}
#install.packages("palmerpenguins")
library(palmerpenguins)
```

Make a scatterplot of two continuous variables of interest: `body_mass_g` and `bill_length_mm`:

```{r}
plot1 <- ggplot(data = penguins) + 
  geom_point(mapping = aes(x = body_mass_g, 
                           y = bill_length_mm), alpha = 0.5) + 
  theme_classic()
plot1
```

Now let's fit a linear model to the data:

```{r}
mod1 <- lm(bill_length_mm ~ body_mass_g,data = penguins)
```

Now, let's look at the model output and its summary output:

```{r}
mod1
summary(mod1)
```

To translate this into a description, use the report function:

```{r}
report(mod1)
```

Visualize the fitted model:

```{r}
mod1 %>% 
  estimate_prediction() %>% 
  plot() + 
  theme_classic()
```

Check the assumptions of the model:

```{r}
check_model(mod1)
```

Let's add species as a factor:

```{r}
mod2 <- lm(bill_length_mm ~ body_mass_g + species,data = penguins)
```

Now, let's look at the model output and its summary output:

```{r}
mod2
summary(mod2)
```

To translate this into a description, use the report function:

```{r}
report(mod2)
```

Visualize the fitted model:

```{r}
mod2 %>% 
  estimate_prediction() %>% 
  plot() + 
  theme_classic()
```

Check the assumptions of the model:

```{r}
check_model(mod2)
```

What if we add island as another factor:

```{r}
mod3 <- lm(bill_length_mm ~ body_mass_g + species + island,data = penguins)
```

Now, let's look at the model output and its summary output:

```{r}
mod3
summary(mod3)
```

Probably not a good idea to include island in this model!

```{r}
report(mod3)
```

Let's compare all 3 models:

```{r}
compare_performance(mod1,mod2,mod3,rank = TRUE)
```

Looks like `mod2` is the best-supported out of these three (lowest RMSE, lowest AIC and BIC, highest performance score).

**PLEASE NOTE: This is just an example analysis to demonstrate how these functions work. Haphazardly adding variables to models and then performing automated model selection is a very bad idea. Building models requires an understanding of statistical assumptions and biological (etc) mechanisms.**

Assuming that you have read the above warning and understand that automatic model selection is not a substitute for statistical understanding -- just for demonstration purposes, let's run mixed effects models using the `lme4` package. First, a random intercept model with `species` as a random effect:

```{r}
#install.packages("lme4")
library(lme4)

# random intercept model
mod4 <- lmer(bill_length_mm ~ body_mass_g + (1|species),data = penguins)
mod4
summary(mod4)
report(mod4)
check_model(mod4)
```

Next, a random intercept and random slopes model, with `species` as a random effect and `body_mass_g` with random slopes:

```{r}
# random intercept + random slopes model
mod5 <- lmer(bill_length_mm ~ body_mass_g + (body_mass_g|species),data = penguins)
mod5
summary(mod5)
report(mod5)
check_model(mod5)
```

And finally, compare the performance of all 5 models:

```{r}
compare_performance(mod1,mod2,mod3,mod4,mod5,rank = TRUE)
```

## Misc examples

```{r}
p1 <- ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_boxplot() +
  theme_modern(axis.text.angle = 45) +
  scale_fill_material_d()

p2 <- ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_violin() +
  theme_modern(axis.text.angle = 45) +
  scale_fill_material_d(palette = "ice")

p3 <- ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Sepal.Length)) +
  geom_point2() +
  theme_modern() +
  scale_color_material_c(palette = "rainbow")

plots(p1, p2, p3, n_columns = 2)

plots(p1, p2, p3,
  n_columns = 2,
  tags = paste("Fig. ", 1:3)
)

ggplot(iris, aes(x = Species, y = Sepal.Length, fill = Species)) +
  geom_violindot(fill_dots = "black") +
  theme_modern() +
  scale_fill_material_d()
```

## Beware of Anscombe's quartet!

A final note -- always be sure to visualize your data. Anscombe's quartet is a famous example or very different datasets that produce **identical** summary statistcs from linear regression.

```{r}
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  print(anova(lmi))
}

## See how close they are (numerically!)
sapply(mods, coef)
lapply(mods, function(fm) coef(summary(fm)))

## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13))
  abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)
```
